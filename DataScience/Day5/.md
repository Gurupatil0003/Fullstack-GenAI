## NlP Basics

# üßπ NLP Text Cleaning & Preprocessing Methods with Sample Code

| #  | Method                       | Purpose                                              | Tool/Library Used           | Example (Before ‚Üí After)            | Sample Code Snippet |
|----|-----------------------------|------------------------------------------------------|-----------------------------|-------------------------------------|----------------------|
| 1  | **Lowercasing**             | Normalize case                                       | `str.lower()`               | "Hello World" ‚Üí "hello world"       | `text.lower()` |
| 2  | **Removing Punctuation**    | Remove unnecessary symbols                           | `re`, `string.punctuation`  | "Hello!!!" ‚Üí "Hello"                | `re.sub(r'[^\w\s]', '', text)` |
| 3  | **Removing Numbers**        | Remove numeric characters                            | `re`                        | "He is 23" ‚Üí "He is"                | `re.sub(r'\d+', '', text)` |
| 4  | **Removing Stopwords**      | Remove frequent, low-info words                      | `nltk`, `spaCy`             | "This is a good example" ‚Üí "good example" | `' '.join([w for w in word_tokenize(text) if w not in stopwords])` |
| 5  | **Tokenization**            | Split sentences into tokens                          | `nltk`, `spaCy`             | "I love AI" ‚Üí ["I", "love", "AI"]   | `word_tokenize(text)` |
| 6  | **Lemmatization**           | Reduce word to base/dictionary form                  | `nltk`, `spaCy`             | "running" ‚Üí "run"                   | `lemmatizer.lemmatize("running")` |
| 7  | **Stemming**                | Reduce word to root by chopping suffixes             | `nltk.PorterStemmer`        | "running" ‚Üí "run"                   | `stemmer.stem("running")` |
| 8  | **Remove Special Characters** | Remove non-alphanumeric characters               | `re`                        | "Hi @#there!" ‚Üí "Hi there"          | `re.sub(r'[^A-Za-z0-9\s]', '', text)` |
| 9  | **Remove Extra Whitespace** | Clean multiple or trailing spaces                    | `re`, `str.strip()`         | "Hello     World" ‚Üí "Hello World"   | `' '.join(text.split())` |
| 10 | **Expand Contractions**     | Expand short forms to full form                      | `contractions`, `re`        | "can't" ‚Üí "cannot"                  | `contractions.fix("can't")` |
| 11 | **Spelling Correction**     | Fix typos or incorrect spellings                     | `textblob`                  | "speling" ‚Üí "spelling"              | `TextBlob("speling").correct()` |
| 12 | **Remove HTML Tags**        | Clean HTML/XML tags from web data                    | `BeautifulSoup`, `re`       | "<p>Hello</p>" ‚Üí "Hello"            | `BeautifulSoup(text, "html.parser").get_text()` |
| 13 | **Remove URLs**             | Remove links                                         | `re`                        | "Visit https://xyz.com" ‚Üí "Visit"   | `re.sub(r'http\S+', '', text)` |
| 14 | **Remove Emojis**           | Remove emojis for clean NLP input                    | `emoji`, `re`               | "I ‚ù§Ô∏è NLP" ‚Üí "I NLP"                | `emoji.replace_emoji(text, '')` |
| 15 | **Remove Emails**           | Remove email addresses                               | `re`                        | "Email me at a@x.com" ‚Üí "Email me"  | `re.sub(r'\S+@\S+', '', text)` |
| 16 | **Remove Mentions**         | Remove @mentions from social data                    | `re`                        | "@user said hi" ‚Üí "said hi"         | `re.sub(r'@\w+', '', text)` |
| 17 | **Remove Hashtags**         | Remove `#` symbol (keep tag word)                    | `re`                        | "#AI is great" ‚Üí "AI is great"      | `re.sub(r'#', '', text)` |
| 18 | **Remove Accents**          | Normalize accented characters                        | `unicodedata`, `unidecode`  | "caf√©" ‚Üí "cafe"                     | `unidecode("caf√©")` |
| 19 | **Text Normalization**      | Convert slang/abbreviations to standard words        | Custom dictionary           | "u", "ur" ‚Üí "you", "your"           | `text.replace("u", "you")` |
| 20 | **POS Tagging (optional)**  | Tag tokens with Part-of-Speech                       | `nltk`, `spaCy`             | "I saw a bear" ‚Üí tagged tokens      | `nltk.pos_tag(word_tokenize(text))` |

---

## üìå Example Cleaning Pipeline (Basic)

```python
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from bs4 import BeautifulSoup
import contractions

def clean_text(text):
    text = text.lower()
    text = contractions.fix(text)
    text = re.sub(r'<.*?>', '', text)  # Remove HTML
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'[^A-Za-z\s]', '', text)  # Remove special characters
    text = ' '.join(text.split())  # Remove extra spaces
    tokens = word_tokenize(text)
    tokens = [t for t in tokens if t not in stopwords.words('english')]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(t) for t in tokens]
    return ' '.join(tokens)
```

## ‚öôÔ∏è Optional Cleaning Techniques (Advanced or Case-Specific)

| #  | Technique                   | Purpose                                               |
|----|-----------------------------|--------------------------------------------------------|
| 21 | Named Entity Removal        | Remove names, locations, orgs from text               |
| 22 | Dependency Parsing          | Understand grammatical structure                      |
| 23 | Sentence Segmentation       | Break text into sentences                             |
| 24 | Case Folding (camel/snake)  | Normalize code identifiers                            |
| 25 | Language Detection & Translation | Handle multilingual datasets                     |

---

## üìå Notes

- Choose cleaning methods **based on your task** ‚Äî not all are always needed.
- Always tokenize **before** lemmatization or stemming.
- For deep learning models (e.g., BERT), **minimal preprocessing** is often better.

---

## üõ†Ô∏è Example Python Libraries

- `nltk`
- `spaCy`
- `textblob`
- `re` (Regular Expressions)
- `BeautifulSoup`
- `unicodedata`, `unidecode`
- `emoji`
- `contractions`

---

## üí° Sample Cleaning Pipeline (Python Pseudocode)

```python
def clean_text(text):
    text = text.lower()
    text = remove_html(text)
    text = remove_urls(text)
    text = expand_contractions(text)
    text = remove_punctuation(text)
    text = remove_numbers(text)
    text = remove_special_characters(text)
    text = remove_stopwords(text)
    text = lemmatize_text(text)
    text = remove_extra_whitespace(text)
    return text
‚ú® Use this as a checklist for any N
```


# üìä Text Analysis Methods (Post-Cleaning) in NLP

| #  | Technique                      | Purpose                                                    | Tool/Library Used       | Output Example                     | Sample Code Snippet |
|----|-------------------------------|-------------------------------------------------------------|-------------------------|------------------------------------|----------------------|
| 1  | **Bag of Words (BoW)**        | Convert text to fixed-size vector of word counts            | `CountVectorizer`       | [1, 0, 2, 1, 0]                    | `CountVectorizer().fit_transform(corpus)` |
| 2  | **TF-IDF**                    | Weigh word importance across documents                      | `TfidfVectorizer`       | [0.2, 0.0, 0.7, 0.3]               | `TfidfVectorizer().fit_transform(corpus)` |
| 3  | **Word Embeddings**           | Dense vector representation capturing semantic similarity   | `Word2Vec`, `spaCy`     | Vector of 300 floats               | `word2vec_model.wv['word']` |
| 4  | **Named Entity Recognition**  | Identify named entities like people, places, organizations  | `spaCy`                 | "Barack Obama" ‚Üí PERSON           | `[(ent.text, ent.label_) for ent in nlp(text).ents]` |
| 5  | **Part-of-Speech Tagging**    | Label each token with grammatical role                      | `nltk`, `spaCy`         | "run" ‚Üí VERB, "book" ‚Üí NOUN       | `nltk.pos_tag(word_tokenize(text))` |
| 6  | **Dependency Parsing**        | Analyze grammatical relationships between words             | `spaCy`                 | Subject ‚Üí Verb ‚Üí Object tree       | `[(tok.text, tok.dep_, tok.head.text) for tok in nlp(text)]` |
| 7  | **Sentiment Analysis**        | Determine positive, negative, or neutral emotion            | `TextBlob`, `VADER`     | Polarity: -1 to +1                 | `TextBlob(text).sentiment.polarity` |
| 8  | **Topic Modeling**            | Discover hidden topics in documents                         | `LDA`, `gensim`         | Topic 1: 60%, Topic 2: 30%, etc.   | `LdaModel(corpus, num_topics=2)` |
| 9  | **Text Summarization**        | Generate concise summaries of long text                     | `sumy`, `transformers`  | Shortened version of the text      | `summarizer(text)` |
| 10 | **Language Detection**        | Identify language of the text                               | `langdetect`, `langid`  | "fr", "en", "hi"                   | `detect("C'est bon")` |

---

## ‚úÖ Example: Vectorization + Sentiment Analysis

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from textblob import TextBlob

corpus = ["I love this place", "I hate traffic", "This is awesome"]
tfidf = TfidfVectorizer()
X = tfidf.fit_transform(corpus)

for doc in corpus:
    blob = TextBlob(doc)
    print(f"Text: {doc} ‚Üí Sentiment: {blob.sentiment.polarity}")
```

```python

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Load dataset
data = pd.read_csv("product_reviews_1000.csv")
texts = data['ReviewText'].astype(str)
ratings = data['Rating'].astype(int)

# Convert ratings to one-hot (e.g. 1-5 ‚Üí [1, 0, 0, 0, 0])
y = to_categorical(ratings - 1)

# Convert text to sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
seqs = tokenizer.texts_to_sequences(texts)
padded = pad_sequences(seqs, padding='post')

# Split data
X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size=0.2)

# Build simple RNN model
model = Sequential([
    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=32),
    SimpleRNN(32),
    Dense(5, activation='softmax')  # 5 rating classes
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5, batch_size=32)

# Evaluate
acc = model.evaluate(X_test, y_test)[1]
print(f"\nüìä Accuracy: {acc:.2f}")

# Predict on sample reviews
samples = ["This phone works great, battery lasts long!",
           "waste.",
           "Average product. Not too bad, not too good."]

sample_seq = tokenizer.texts_to_sequences(samples)
sample_pad = pad_sequences(sample_seq, maxlen=padded.shape[1], padding='post')
preds = model.predict(sample_pad)

for review, p in zip(samples, preds):
    print(f"\nüìù Review: {review}")
    print(f"‚≠ê Predicted Rating: {np.argmax(p) + 1}")

```

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Load data
data = pd.read_csv("all_indian_language_reviews_1000.csv")
texts = data['ReviewText'].astype(str)
languages = data['Language']

# Convert language names to numbers
encoder = LabelEncoder()
labels = encoder.fit_transform(languages)
labels = to_categorical(labels)

# Convert text to numbers
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded = pad_sequences(sequences, padding='post')

# Split data
X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2)

# Build simple model
model = Sequential([
    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=32),  # smaller embedding
    SimpleRNN(32),  # smaller RNN
    Dense(labels.shape[1], activation='softmax')  # output layer
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5, batch_size=32)

# Evaluate
acc = model.evaluate(X_test, y_test)[1]
print(f"\n‚úÖ Accuracy: {acc:.2f}")

# Test on a new sentence
sample = ["‡¶Ü‡¶Æ‡¶ø ‡¶ñ‡ßÅ‡¶¨ ‡¶∏‡¶®‡ßç‡¶§‡ßÅ‡¶∑‡ßç‡¶ü‡•§"]  # Bengali text
sample_seq = tokenizer.texts_to_sequences(sample)
sample_pad = pad_sequences(sample_seq, maxlen=padded.shape[1], padding='post')
predicted = encoder.inverse_transform([model.predict(sample_pad).argmax()])
print("üîÆ Predicted Language:", predicted[0])



```
