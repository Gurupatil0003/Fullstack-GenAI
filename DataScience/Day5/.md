```python

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Load dataset
data = pd.read_csv("product_reviews_1000.csv")
texts = data['ReviewText'].astype(str)
ratings = data['Rating'].astype(int)

# Convert ratings to one-hot (e.g. 1-5 ‚Üí [1, 0, 0, 0, 0])
y = to_categorical(ratings - 1)

# Convert text to sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
seqs = tokenizer.texts_to_sequences(texts)
padded = pad_sequences(seqs, padding='post')

# Split data
X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size=0.2)

# Build simple RNN model
model = Sequential([
    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=32),
    SimpleRNN(32),
    Dense(5, activation='softmax')  # 5 rating classes
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5, batch_size=32)

# Evaluate
acc = model.evaluate(X_test, y_test)[1]
print(f"\nüìä Accuracy: {acc:.2f}")

# Predict on sample reviews
samples = ["This phone works great, battery lasts long!",
           "waste.",
           "Average product. Not too bad, not too good."]

sample_seq = tokenizer.texts_to_sequences(samples)
sample_pad = pad_sequences(sample_seq, maxlen=padded.shape[1], padding='post')
preds = model.predict(sample_pad)

for review, p in zip(samples, preds):
    print(f"\nüìù Review: {review}")
    print(f"‚≠ê Predicted Rating: {np.argmax(p) + 1}")

```

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Load data
data = pd.read_csv("all_indian_language_reviews_1000.csv")
texts = data['ReviewText'].astype(str)
languages = data['Language']

# Convert language names to numbers
encoder = LabelEncoder()
labels = encoder.fit_transform(languages)
labels = to_categorical(labels)

# Convert text to numbers
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded = pad_sequences(sequences, padding='post')

# Split data
X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2)

# Build simple model
model = Sequential([
    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=32),  # smaller embedding
    SimpleRNN(32),  # smaller RNN
    Dense(labels.shape[1], activation='softmax')  # output layer
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5, batch_size=32)

# Evaluate
acc = model.evaluate(X_test, y_test)[1]
print(f"\n‚úÖ Accuracy: {acc:.2f}")

# Test on a new sentence
sample = ["‡¶Ü‡¶Æ‡¶ø ‡¶ñ‡ßÅ‡¶¨ ‡¶∏‡¶®‡ßç‡¶§‡ßÅ‡¶∑‡ßç‡¶ü‡•§"]  # Bengali text
sample_seq = tokenizer.texts_to_sequences(sample)
sample_pad = pad_sequences(sample_seq, maxlen=padded.shape[1], padding='post')
predicted = encoder.inverse_transform([model.predict(sample_pad).argmax()])
print("üîÆ Predicted Language:", predicted[0])



```
