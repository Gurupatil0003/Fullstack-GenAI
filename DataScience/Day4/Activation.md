# âš¡ Activation Functions in Deep Learning

Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns. Here's a comparison of commonly used activation functions.

## ðŸ”¢ Activation Functions Comparison Table

| Activation Function | Equation (Simplified)       | Range           | Differentiable | Pros                                                    | Cons                                                   | Use Cases                              |
|---------------------|-----------------------------|------------------|----------------|---------------------------------------------------------|--------------------------------------------------------|----------------------------------------|
| **Sigmoid**         | 1 / (1 + e^(-x))            | (0, 1)           | âœ…              | Smooth curve, good for probability                      | Vanishing gradient, slow convergence                   | Binary classification (output layer)   |
| **Tanh**            | (e^x - e^(-x)) / (e^x + e^(-x)) | (-1, 1)       | âœ…              | Zero-centered, better than sigmoid                      | Vanishing gradient, slow learning                      | Hidden layers in shallow networks      |
| **ReLU**            | max(0, x)                   | [0, âˆž)           | âœ…              | Fast, simple, sparsity                                 | Dying ReLU problem                                     | CNNs, general hidden layers            |
| **Leaky ReLU**      | x if x > 0 else Î±x          | (-âˆž, âˆž)          | âœ…              | Solves dying ReLU, allows small gradients               | May still be unstable                                 | Deep networks with dying ReLU issue    |
| **Parametric ReLU** | x if x > 0 else a*x         | (-âˆž, âˆž)          | âœ…              | Learnable slope for negative inputs                    | Risk of overfitting if not regularized                 | Advanced deep networks                 |
| **ELU**             | x if x > 0 else Î±*(e^x - 1) | (-Î±, âˆž)          | âœ…              | Zero mean outputs, avoids dying neurons                 | Slower computation than ReLU                          | Complex networks, image tasks          |
| **SELU**            | Î» * ELU(x)                  | (-âˆž, âˆž)          | âœ…              | Self-normalizing networks                              | Requires careful initialization and architecture       | Self-normalizing deep nets             |
| **Swish**           | x * sigmoid(x)              | (-0.28, âˆž)       | âœ…              | Smooth, non-monotonic, performs better in some models   | Slower to compute                                     | Transformers, deep CNNs, BERT          |
| **GELU**            | x * Î¦(x)                    | (-âˆž, âˆž)          | âœ…              | Better than ReLU in some models, smooth                | Computation-intensive                                 | BERT, Transformers, NLP                |
| **Softmax**         | e^(xáµ¢) / Î£ e^(xâ±¼)           | (0, 1) (sum=1)   | âœ…              | Converts logits to probabilities                       | Not used in hidden layers                             | Multi-class classification (output)    |
| **Hard Sigmoid**    | Approx. linear around 0     | [0, 1]           | âœ…              | Fast approximation of sigmoid                          | Less accurate, not smooth                             | Lightweight models, mobile devices     |
| **Hard Swish**      | x * ReLU6(x+3)/6            | ~(-0.25, âˆž)      | âœ…              | Fast & efficient Swish approximation                   | Less precise than Swish                               | MobileNetV3, edge devices              |

---

## ðŸ§  Summary

- **Use ReLU** for most hidden layers (fast & effective).
- **Use Softmax** in output layer for multi-class classification.
- **Use Sigmoid** in output layer for binary classification.
- **Use GELU / Swish** in Transformer-based models.
- **Use Leaky ReLU or ELU** to fix dying ReLU issues.

---

## ðŸ“Œ Notes

- Smooth â‰  always better. Choose based on task and network architecture.
- Some activations (Swish, GELU) perform better with **large-scale models**.
- Always **normalize inputs** and use **proper weight initialization** to reduce vanishing gradient issues.

