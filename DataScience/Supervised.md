# Datascience Notebook Examples

| Topic            | PDF Link                                                                                                                                     | Streamlit App                                                                                      | Colab Notebook                                                                                                                                           |
|------------------|----------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|
| Classification     | <a href="PDF_LINK_HERE" target="_parent"><img src="https://img.shields.io/badge/Open in PDF-%23FF0000.svg?style=flat-square&logo=adobe&logoColor=white"/></a> | <a href="STREAMLIT_LINK_HERE" target="_parent"><img src="https://static.streamlit.io/badges/streamlit_badge_black_white.svg"/></a> | <a href="https://colab.research.google.com/drive/1F3z64bjBCmw7qmjAmUrtt0a4xo-7k_hs?usp=sharing" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
| Regression    | <a href="PDF_LINK_HERE" target="_parent"><img src="https://img.shields.io/badge/Open in PDF-%23FF0000.svg?style=flat-square&logo=adobe&logoColor=white"/></a> | <a href="STREAMLIT_LINK_HERE" target="_parent"><img src="https://static.streamlit.io/badges/streamlit_badge_black_white.svg"/></a> | <a href="https://colab.research.google.com/drive/1nePFBkd0SOFDsTUaGo7c2vM-D2O3Q4F-?usp=sharing" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
| Churn     | <a href="PDF_LINK_HERE" target="_parent"><img src="https://img.shields.io/badge/Open in PDF-%23FF0000.svg?style=flat-square&logo=adobe&logoColor=white"/></a> | <a href="STREAMLIT_LINK_HERE" target="_parent"><img src="https://static.streamlit.io/badges/streamlit_badge_black_white.svg"/></a> | <a href="https://colab.research.google.com/drive/1FMrPMla0SNmgYfAsP9hcx6Llu98u98-U?usp=sharing" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
| Matplotlib    | <a href="PDF_LINK_HERE" target="_parent"><img src="https://img.shields.io/badge/Open in PDF-%23FF0000.svg?style=flat-square&logo=adobe&logoColor=white"/></a> | <a href="STREAMLIT_LINK_HERE" target="_parent"><img src="https://static.streamlit.io/badges/streamlit_badge_black_white.svg"/></a> | <a href="https://colab.research.google.com/drive/1STdP8lBpbyREeiPmTlXWMZPMCI6gjchC?usp=sharing" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
| Plotly     | <a href="PDF_LINK_HERE" target="_parent"><img src="https://img.shields.io/badge/Open in PDF-%23FF0000.svg?style=flat-square&logo=adobe&logoColor=white"/></a> | <a href="STREAMLIT_LINK_HERE" target="_parent"><img src="https://static.streamlit.io/badges/streamlit_badge_black_white.svg"/></a> | <a href="https://colab.research.google.com/drive/1USNV4joQrFp81fvP__T4-9W7S5nhm9NA?usp=sharing" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
| Rent_car     | <a href="PDF_LINK_HERE" target="_parent"><img src="https://img.shields.io/badge/Open in PDF-%23FF0000.svg?style=flat-square&logo=adobe&logoColor=white"/></a> | <a href="STREAMLIT_LINK_HERE" target="_parent"><img src="https://static.streamlit.io/badges/streamlit_badge_black_white.svg"/></a> | <a href="https://colab.research.google.com/drive/1RLiZqCfhawULkwLyz92AwtkBrJEkdhEu?usp=sharing#scrollTo=mqyiuJReow1E" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="https://colab.research.google.com/drive/1FTdJ8t7mgirUClfPMMOPOAt6m1tZesRh?usp=sharing"/></a> |




ğŸ“Œ What is Supervised Learning?
Definition:
Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset â€” meaning each training example is paired with an output label. The goal is to learn a function that maps inputs to outputs.

ğŸ§  Supervised Learning Workflow:
Input Features (X) â†’ Passed to the model

Target Labels (Y) â†’ Used to guide learning

Model â†’ Learns mapping f(X) â‰ˆ Y

Prediction â†’ Model outputs predicted labels for unseen data

Loss Function â†’ Measures prediction error

Optimization â†’ Adjust model to minimize the loss

ğŸ§© Common Supervised Learning Models:
Letâ€™s go one by one:

1. Linear Regression
ğŸ”¹ Type: Regression

Definition:
Predicts a continuous output using a linear combination of input features.

Mathematics:

ğ‘¦
=
ğ‘¤
0
+
ğ‘¤
1
ğ‘¥
1
+
ğ‘¤
2
ğ‘¥
2
+
â‹¯
+
ğ‘¤
ğ‘›
ğ‘¥
ğ‘›
y=w 
0
â€‹
 +w 
1
â€‹
 x 
1
â€‹
 +w 
2
â€‹
 x 
2
â€‹
 +â‹¯+w 
n
â€‹
 x 
n
â€‹
 
Where:

ğ‘¦
y = predicted output

ğ‘¤
ğ‘–
w 
i
â€‹
  = learned weights

Loss Function:
Mean Squared Error (MSE):

MSE
=
1
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
(
ğ‘¦
ğ‘–
âˆ’
ğ‘¦
^
ğ‘–
)
2
MSE= 
n
1
â€‹
  
i=1
âˆ‘
n
â€‹
 (y 
i
â€‹
 âˆ’ 
y
^
â€‹
  
i
â€‹
 ) 
2
 
Prediction Logic:
Draws a straight line (or hyperplane) to best fit the training data.

Use When:

Relationship between variables is linear

You want interpretable models

2. Logistic Regression
ğŸ”¹ Type: Classification (Binary/Multiclass)

Definition:
Used to predict categorical outcomes (like Yes/No, Spam/Not Spam).

Mathematics:

ğ‘ƒ
(
ğ‘¦
=
1
âˆ£
ğ‘¥
)
=
ğœ
(
ğ‘¤
ğ‘‡
ğ‘¥
+
ğ‘
)
=
1
1
+
ğ‘’
âˆ’
(
ğ‘¤
ğ‘‡
ğ‘¥
+
ğ‘
)
P(y=1âˆ£x)=Ïƒ(w 
T
 x+b)= 
1+e 
âˆ’(w 
T
 x+b)
 
1
â€‹
 
Loss Function:
Binary Cross-Entropy:

âˆ’
1
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
[
ğ‘¦
ğ‘–
log
â¡
(
ğ‘¦
^
ğ‘–
)
+
(
1
âˆ’
ğ‘¦
ğ‘–
)
log
â¡
(
1
âˆ’
ğ‘¦
^
ğ‘–
)
]
âˆ’ 
n
1
â€‹
  
i=1
âˆ‘
n
â€‹
 [y 
i
â€‹
 log( 
y
^
â€‹
  
i
â€‹
 )+(1âˆ’y 
i
â€‹
 )log(1âˆ’ 
y
^
â€‹
  
i
â€‹
 )]
Prediction Logic:
Uses sigmoid function to squash output into probability range [0, 1].

Use When:

You have a binary classification task

You want a fast, simple model with probabilistic output

3. Decision Tree
ğŸ”¹ Type: Classification / Regression

Definition:
Tree-based structure that splits data based on feature thresholds to make decisions.

Mathematics:
Splits are made using metrics like:

Gini Impurity: 
ğº
=
1
âˆ’
âˆ‘
ğ‘
ğ‘–
2
G=1âˆ’âˆ‘p 
i
2
â€‹
 

Entropy: 
ğ»
=
âˆ’
âˆ‘
ğ‘
ğ‘–
log
â¡
2
(
ğ‘
ğ‘–
)
H=âˆ’âˆ‘p 
i
â€‹
 log 
2
â€‹
 (p 
i
â€‹
 )

MSE (for regression)

Prediction Logic:
Follows decision nodes until it reaches a leaf node (label).

Use When:

Data has clear decision rules

You want interpretability and non-linearity

4. Random Forest
ğŸ”¹ Type: Classification / Regression
ğŸ”¹ Ensemble of Decision Trees

Definition:
Combines multiple decision trees to improve accuracy and reduce overfitting.

Mathematics:

Aggregates outputs from multiple trees (majority vote for classification, average for regression)

Prediction Logic:
Each tree votes; result is average or mode.

Use When:

You want high accuracy

You need robustness to overfitting

5. Support Vector Machine (SVM)
ğŸ”¹ Type: Classification / Regression (SVR)

Definition:
Finds the optimal hyperplane that maximally separates data into classes.

Mathematics:

MaximizeÂ marginÂ 
â‡’
2
âˆ£
âˆ£
ğ‘¤
âˆ£
âˆ£
MaximizeÂ marginÂ â‡’ 
âˆ£âˆ£wâˆ£âˆ£
2
â€‹
 
Subject to:

ğ‘¦
ğ‘–
(
ğ‘¤
â‹…
ğ‘¥
ğ‘–
+
ğ‘
)
â‰¥
1
y 
i
â€‹
 (wâ‹…x 
i
â€‹
 +b)â‰¥1
Kernel Trick: Allows non-linear separation using functions like RBF, Polynomial.

Prediction Logic:
Classifies based on which side of the hyperplane the data lies.

Use When:

You need robust classifier with small datasets

High-dimensional data

6. k-Nearest Neighbors (k-NN)
ğŸ”¹ Type: Classification / Regression

Definition:
Instance-based method; classifies data points based on the majority vote of k nearest neighbors.

Mathematics:
Distance metrics:

Euclidean: 
ğ‘‘
(
ğ‘¥
,
ğ‘¥
â€²
)
=
âˆ‘
(
ğ‘¥
ğ‘–
âˆ’
ğ‘¥
ğ‘–
â€²
)
2
d(x,x 
â€²
 )= 
âˆ‘(x 
i
â€‹
 âˆ’x 
i
â€²
â€‹
 ) 
2
 
â€‹
 

Manhattan, Cosine, etc.

Prediction Logic:
No training; during prediction, checks nearest neighbors in training data.

Use When:

Data is small

You want a simple, non-parametric model

7. Naive Bayes
ğŸ”¹ Type: Classification

Definition:
Probabilistic model based on Bayesâ€™ theorem assuming feature independence.

Mathematics:

ğ‘ƒ
(
ğ‘¦
âˆ£
ğ‘¥
1
,
.
.
.
,
ğ‘¥
ğ‘›
)
âˆ
ğ‘ƒ
(
ğ‘¦
)
âˆ
ğ‘–
=
1
ğ‘›
ğ‘ƒ
(
ğ‘¥
ğ‘–
âˆ£
ğ‘¦
)
P(yâˆ£x 
1
â€‹
 ,...,x 
n
â€‹
 )âˆP(y) 
i=1
âˆ
n
â€‹
 P(x 
i
â€‹
 âˆ£y)
Prediction Logic:
Chooses class with highest posterior probability.

Use When:

Text classification (spam detection)

Data is high-dimensional and categorical

8. Gradient Boosting (XGBoost, LightGBM, etc.)
ğŸ”¹ Type: Classification / Regression
ğŸ”¹ Ensemble of Trees

Definition:
Builds trees sequentially, each correcting the errors of the previous.

Mathematics:
Minimizes loss:

PredictionÂ 
=
âˆ‘
ğ‘š
=
1
ğ‘€
ğ›¾
ğ‘š
â„
ğ‘š
(
ğ‘¥
)
PredictionÂ = 
m=1
âˆ‘
M
â€‹
 Î³ 
m
â€‹
 h 
m
â€‹
 (x)
where 
â„
ğ‘š
h 
m
â€‹
  is the m-th weak learner (typically a decision tree)

Loss Functions:

MSE (regression)

Log loss (classification)

Prediction Logic:
Each tree corrects the residuals of the last.

Use When:

You want state-of-the-art performance

You need to handle structured/tabular data

9. Artificial Neural Networks (ANN)
ğŸ”¹ Type: Classification / Regression

Definition:
Mimics the human brain using layers of neurons to learn complex functions.

Mathematics:
Forward pass:

ğ‘
(
ğ‘™
)
=
ğ‘“
(
ğ‘Š
(
ğ‘™
)
ğ‘
(
ğ‘™
âˆ’
1
)
+
ğ‘
(
ğ‘™
)
)
a 
(l)
 =f(W 
(l)
 a 
(lâˆ’1)
 +b 
(l)
 )
Loss minimized via backpropagation using gradient descent.

Prediction Logic:
Activations flow forward; loss is backpropagated to update weights.

Use When:

Data is non-linear and large-scale

You want flexibility and deep architectures

âœ… When and Why to Use Supervised Models:
Scenario	Model Suggestion	Reason
Predicting housing prices	Linear Regression	Continuous output, linear
Email spam detection	Naive Bayes, Logistic Regression	Text classification
Disease diagnosis	Random Forest, SVM	Non-linear, robust models
Customer churn prediction	XGBoost, Logistic Regression	Accuracy + Interpretability
Image classification (basic)	ANN, SVM	Non-linear, scalable
Product recommendation (simple)	k-NN	Memory-based similarity
Sentiment analysis	Naive Bayes, Logistic Regression	Text with categorical labels

ğŸ”§ Bonus: Evaluation Metrics
Accuracy

Precision / Recall / F1 Score

ROC-AUC

Confusion Matrix

RÂ² Score (Regression)

